# Kubernetes Cluster Setup with Timeweb Cloud and CloudFlare

Этот проект содержит Terraform-конфигурацию для создания кластера Kubernetes в Timeweb Cloud с интеграцией CloudFlare для управления DNS и R2 для хранения состояния Terraform.

## Архитектура кластера

- **1 мастер-нода**: Используется первый доступный пресет типа "master"
- **2 воркер-ноды**: Используется первый доступный пресет типа "worker"
- **VPC сеть**: 10.100.0.0/16 для изоляции
- **Floating IP**: Для внешнего доступа к API кластера
- **CloudFlare интеграция**: Автоматическое создание DNS-записей
- **R2 хранилище**: S3-совместимое хранилище CloudFlare для хранения tfstate

## Предварительные требования

1. **Timeweb Cloud аккаунт** с API-токеном
2. **CloudFlare аккаунт** с доменом и R2 Bucket
3. **Terraform** версии 0.13 или выше
4. **kubectl** для управления кластером
5. **Доступ к R2** с Access Key ID и Secret Access Key

## Настройка

### 1. Клонируйте репозиторий и перейдите в директорию

```bash
cd timeweb-cloud-terraform
```

### 2. Настройте переменные для CloudFlare

**Вариант 1: Через переменные окружения (рекомендуется для production)**

```bash
export CLOUDFLARE_API_TOKEN="ваш_cloudflare_api_token"
export CLOUDFLARE_EMAIL="ваш_email@domain.com"
```

**Вариант 2: Через файл (для локального тестирования)**

Создайте файл `cloudflare.auto.tfvars`:

```hcl
cloudflare_api_token = "ваш_cloudflare_api_token"
cloudflare_email    = "ваш_email@domain.com"
```

⚠️ **Важно**: Добавьте `cloudflare.auto.tfvars` в `.gitignore`, чтобы не.commitить токены в репозиторий:

```bash
echo "cloudflare.auto.tfvars" >> .gitignore
```

### 3. Получите CloudFlare API Token

1. Зайдите в [CloudFlare Dashboard](https://dash.cloudflare.com/)
2. Перейдите в **My Profile** → **API Tokens**
3. Создайте новый токен с правами:
   - Zone.Zone: Read
   - Zone.DNS: Edit
4. Скопируйте токен и используйте в переменных окружения или файле `cloudflare.auto.tfvars`

### 4. Настройка R2 для хранения tfstate

Для хранения состояния Terraform используется CloudFlare R2 - S3-совместимое хранилище.

#### 4.1 Создание R2 Bucket

1. Зайдите в [CloudFlare Dashboard](https://dash.cloudflare.com/)
2. Перейдите в раздел **R2**
3. Создайте новый Bucket с именем `terraform-state` (или другим, соответствующим вашей конфигурации)
4. Запомните имя вашего Bucket

#### 4.2 Получение учетных данных R2

1. В CloudFlare Dashboard перейдите в раздел **R2**
2. Нажмите "Manage R2 API Tokens"
3. Создайте или используйте существующие Access Key ID и Secret Access Key

#### 4.3 Настройка локальной конфигурации

Создайте файл `r2-backend.conf` для передачи учетных данных при инициализации Terraform:

```bash
# Учетные данные R2
endpoint = "https://<account_id>.r2.cloudflarestorage.com"
access_key = "<r2_access_key_id>"
secret_key = "<r2_secret_access_key>"
bucket = "terraform-state"
key = "kubernetes/terraform.tfstate"
region = "auto"
skip_credentials_validation = true
skip_region_validation      = true
skip_metadata_api_check     = true
use_path_style              = true
```

### 5. Настройка безопасности учетных данных в GitHub

Для безопасного хранения учетных данных в GitHub Actions:

1. В настройках репозитория перейдите в раздел Secrets and variables → Actions
2. Добавьте следующие секреты:
   - `CLOUDFLARE_API_TOKEN`: ваш CloudFlare API токен
   - `CLOUDFLARE_ACCOUNT_ID`: ваш CloudFlare Account ID
   - `R2_ACCESS_KEY_ID`: ваш R2 Access Key ID
   - `R2_SECRET_ACCESS_KEY`: ваш R2 Secret Access Key
   - `TIMEWEB_TOKEN`: ваш Timeweb Cloud API токен

### 6. Настройте переменные в `variables.tf`

Отредактируйте файл `variables.tf` и укажите ваш домен:

```hcl
variable "domain_name" {
  description = "Your domain name for DNS records"
  type        = string
  default     = "vovanbl411.qzz.io"  # Замените на ваш домен
}
```

### 7. Проверьте токен Timeweb Cloud

Убедитесь, что файл `token.txt` содержит ваш API-токен Timeweb Cloud.

## Развертывание кластера

### 1. Инициализация Terraform с R2 бэкендом

Перед первой инициализацией убедитесь, что у вас есть учетные данные R2:

```bash
terraform init -backend-config=r2-backend.conf
```

### 2. Проверка плана

```bash
terraform plan
```

### 3. Применение конфигурации

```bash
terraform apply
```

Применение может занять 10-15 минут. Terraform будет создавать:
- VPC сеть
- Kubernetes кластер с мастер-нодой
- Группу воркер-нод (2 ноды)
- Floating IP адрес
- DNS-записи в CloudFlare
- Состояние будет сохранено в R2

### 4. Получение kubeconfig

После успешного применения получите kubeconfig:

```bash
terraform output kubeconfig > kubeconfig.yaml
```

### 5. Настройка kubectl

```bash
export KUBECONFIG=./kubeconfig.yaml
kubectl cluster-info
```

### 6. Проверка состояния кластера

```bash
kubectl get nodes
kubectl get pods -A
```

## DNS Настройка

После развертывания будут созданы следующие DNS-записи в CloudFlare:

- `k8s-api.vovanbl411.qzz.io` → Floating IP кластера (для доступа к Kubernetes API)
- `*.apps.vovanbl411.qzz.io` → Floating IP кластера (для приложений)

## Установка Ingress Controller

Для маршрутизации внешнего трафика установите NGINX Ingress Controller:

### Вариант 1: Использование скрипта (рекомендуется)

```bash
chmod +x install-ingress.sh
# Отредактируйте install-ingress.sh и замените YOUR_FLOATING_IP на ваш floating IP
./install-ingress.sh
```

### Вариант 2: Ручная установка

```bash
# Добавление репозитория Helm
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

# Установка с вашим floating IP
FLOATING_IP=$(terraform output floating_ip)
helm install nginx-ingress ingress-nginx/ingress-nginx \
    --namespace ingress-nginx \
    --create-namespace \
    --set controller.service.externalIPs={$FLOATING_IP} \
    --wait
```

Проверьте установку:

```bash
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx
```

## Развертывание тестового приложения

Создайте файл `test-app.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-app
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test-app
  template:
    metadata:
      labels:
        app: test-app
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: test-app-service
  namespace: default
spec:
  selector:
    app: test-app
  ports:
    - port: 80
      targetPort: 80
  type: ClusterIP

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-app-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: test.apps.vovanbl411.qzz.io
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: test-app-service
            port:
              number: 80
```

Примените:

```bash
kubectl apply -f test-app.yaml
```

## SSL-сертификаты

### Вариант 1: CloudFlare Origin Certificate

1. В CloudFlare Dashboard перейдите в **SSL/TLS** → **Origin Server**
2. Создайте сертификат
3. Создайте секрет в Kubernetes:

```bash
kubectl create secret tls cloudflare-cert --cert=path/to/cert.pem --key=path/to/private.key
```

### Вариант 2: Let's Encrypt с cert-manager

Установите cert-manager:

```bash
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.2/cert-manager.yaml
```

Создайте ClusterIssuer для Let's Encrypt.

## Мониторинг

Установите Prometheus и Grafana:

```bash
kubectl create namespace monitoring
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring
```

## Управление кластером

### Масштабирование воркер-нод

В файле `k8s-cluster.tf` измените `node_count`:

```hcl
resource "twc_k8s_node_group" "worker_nodes" {
  # ...
  node_count = 3  # Измените количество нод
}
```

Примените изменения:

```bash
terraform apply
```

### Обновление версии Kubernetes

Измените версию в `variables.tf`:

```hcl
variable "k8s_version" {
  description = "Kubernetes version"
  type        = string
 default     = "v1.34.2"  # Новая версия
}
```

### Автомасштабирование

Включите автомасштабирование:

```hcl
resource "twc_k8s_node_group" "worker_nodes" {
  # ...
  is_autoscaling = true
  min_size = 2
  max_size = 5
}
```

## Удаление кластера

```bash
terraform destroy
```

## Работа с R2 бэкендом

### Миграция с локального бэкенда

Если у вас уже есть локальный tfstate файл, и вы хотите перенести его в R2, выполните следующие шаги:

1. Временно измените конфигурацию бэкенда в `backend.tf` на локальную:
```hcl
terraform {
  backend "local" {
    path = "terraform.tfstate"
  }
}
```

2. Выполните `terraform init` для загрузки локального состояния

3. Верните конфигурацию R2 в `backend.tf`

4. Снова выполните `terraform init` и подтвердите миграцию состояния

### Синхронизация состояния

Если вы работаете в команде, всегда синхронизируйте состояние перед применением изменений:

```bash
terraform refresh
```

### Резервное копирование состояния

Состояние автоматически сохраняется в R2, но вы можете экспортировать его вручную:

```bash
terraform state pull > terraform.tfstate.backup
```

## CI/CD Интеграция с GitHub Actions

Файл `.github/workflows/terraform.yml` уже создан в репозитории и содержит конфигурацию для автоматизации развертывания через GitHub Actions:

```yaml
name: Terraform Deploy

on:
  push:
    branches: [main]
  pull_request:

jobs:
  terraform:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: 1.12.1
        
    - name: Configure AWS credentials for R2
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.R2_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        aws-region: auto
        
    - name: Terraform Init
      run: |
        terraform init \
          -backend-config="endpoint=https://${{ secrets.CLOUDFLARE_ACCOUNT_ID }}.r2.cloudflarestorage.com" \
          -backend-config="bucket=terraform-state" \
          -backend-config="access_key=${{ secrets.R2_ACCESS_KEY_ID }}" \
          -backend-config="secret_key=${{ secrets.R2_SECRET_ACCESS_KEY }}" \
          -backend-config="region=auto" \
          -backend-config="skip_credentials_validation=true" \
          -backend-config="skip_region_validation=true" \
          -backend-config="skip_metadata_api_check=true" \
          -backend-config="use_path_style=true"
      
    - name: Terraform Plan
      run: terraform plan
      env:
        CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        
    - name: Terraform Apply
      if: github.ref == 'refs/heads/main'
      run: terraform apply -auto-approve
      env:
        CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        TIMEWEB_TOKEN: ${{ secrets.TIMEWEB_TOKEN }}
```

## Troubleshooting

### Проблемы с подключением

1. Проверьте kubeconfig:
   ```bash
   kubectl cluster-info
   ```

2. Проверьте статус нод:
   ```bash
   kubectl get nodes
   kubectl describe node <node-name>
   ```

### DNS проблемы

1. Проверьте DNS-записи в CloudFlare Dashboard
2. Используйте `nslookup` для проверки разрешения:
   ```bash
   nslookup k8s-api.vovanbl411.qzz.io
   ```

### Проблемы с API токенами

1. Убедитесь, что токены не истекли
2. Проверьте права доступа токенов
3. Пересоздайте токены при необходимости

### Проблемы с R2 бэкендом

1. Проверьте наличие учетных данных R2:
   ```bash
   echo $AWS_ACCESS_KEY_ID
   echo $AWS_SECRET_ACCESS_KEY
   ```
   
2. Убедитесь, что bucket существует и доступен
3. Проверьте права доступа к bucket

## Стоимость

Примерная стоимость в месяц (на основе тарифов Timeweb Cloud):
- Мастер-нода: Зависит от выбранного пресета (первый доступный пресет типа "master")
- 2 воркер-ноды: Зависит от выбранного пресета (первый доступный пресет типа "worker")
- Floating IP: ~50 руб/месяц
- R2 Storage: Зависит от объема хранения и трафика (см. тарифы CloudFlare R2)
- Общий трафик: зависит от использования

## Поддержка

При возникновении проблем:
1. Проверьте логи Terraform: `terraform apply -debug`
2. Проверьте статус ресурсов в Timeweb Cloud панели
3. Проверьте логи Kubernetes: `kubectl logs -n kube-system`
4. Проверьте доступность R2 bucket и учетных данных

## Следующие шаги

1. Настройте CI/CD пайплайн с GitHub Actions
2. Добавьте мониторинг и алертинг
3. Настройте бэкапы кластера
4. Рассмотрите использование managed баз данных
5. Настройте логирование (ELK stack)